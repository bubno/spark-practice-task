{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b4003b6-35d0-4404-980d-07ed10cd4683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|          city|   lat|    lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "|197568495625|          10|    The Golden Spoon|                  24784|     US|       Decatur|34.578|-87.021|\n",
      "| 17179869242|          59|         Azalea Cafe|                  10902|     FR|         Paris|48.861|  2.368|\n",
      "|214748364826|          27|     The Corner Cafe|                  92040|     US|    Rapid City| 44.08|-103.25|\n",
      "|154618822706|          51|        The Pizzeria|                  41484|     AT|        Vienna|48.213| 16.413|\n",
      "|163208757312|          65|       Chef's Corner|                  96638|     GB|        London|51.495| -0.191|\n",
      "| 68719476763|          28|    The Spicy Pickle|                  77517|     US|      Grayling|44.657|-84.744|\n",
      "|223338299419|          28|    The Spicy Pickle|                  36937|     US|        Oswego|43.452|-76.532|\n",
      "|240518168650|          75|     Greenhouse Cafe|                  93164|     NL|     Amsterdam| 52.37|  4.897|\n",
      "|128849018936|          57|The Yellow Submarine|                   5679|     FR|         Paris|48.872|  2.335|\n",
      "|197568495635|          20|       The Brasserie|                  24784|     US|Jeffersonville|39.616|-83.612|\n",
      "| 68719476768|          33|   The Blue Elephant|                  77517|     IT|         Milan|45.479|  9.146|\n",
      "| 51539607582|          31|           Bistro 42|                   6934|     IT|         Milan|45.444|  9.153|\n",
      "| 94489280554|          43|      The Food House|                  95399|     FR|         Paris|48.867|  2.329|\n",
      "|206158430215|           8|     The Green Olive|                  53370|     US|   Haltom City|32.789| -97.28|\n",
      "|154618822657|           2|        Bella Cucina|                  41484|     US|   Fort Pierce|27.412|-80.391|\n",
      "| 17179869217|          34|     The Tasty Treat|                  10902|     US|     Green Bay|44.476|-88.077|\n",
      "|  8589934633|          42|     The Daily Scoop|                  12630|     FR|         Paris|48.854|  2.343|\n",
      "|240518168596|          21|      The Lazy Daisy|                  93164|     US|    Mendenhall| 39.86|-75.646|\n",
      "|171798691906|          67|  Crimson and Clover|                  65939|     NL|     Amsterdam|52.361|  4.894|\n",
      "| 42949673022|          63|          Cafe Paris|                  89646|     GB|        London|51.508| -0.107|\n",
      "+------------+------------+--------------------+-----------------------+-------+--------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read CSV Example\").getOrCreate()\n",
    "\n",
    "# Define the correct path to the directory containing your CSV files\n",
    "path_to_csv = \"/home/jovyan/work/PT-Spark/restaurant_csv/\"\n",
    "\n",
    "# Read all CSV files from the directory into a DataFrame\n",
    "restaurant_df  = spark.read.csv(path_to_csv, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the data was read correctly\n",
    "restaurant_df .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22ecc1a0-eb6f-4277-889c-f4cbe7a695a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кількість NULL значень в колонці id: 0\n",
      "Кількість NULL значень в колонці franchise_id: 0\n",
      "Кількість NULL значень в колонці franchise_name: 0\n",
      "Кількість NULL значень в колонці restaurant_franchise_id: 0\n",
      "Кількість NULL значень в колонці country: 0\n",
      "Кількість NULL значень в колонці city: 0\n",
      "Кількість NULL значень в колонці lat: 1\n",
      "Кількість NULL значень в колонці lng: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate over all columns and count NULL values for each column\n",
    "for column in restaurant_df.columns:\n",
    "    nulls_count = restaurant_df.filter(col(column).isNull()).count()\n",
    "    print(f\"Кількість NULL значень в колонці {column}: {nulls_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c0d25e4-7637-48fe-abbf-cca75fb5f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|NULL|NULL|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurant_df .filter(col(\"lat\").isNull() | col(\"lng\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67147442-75cd-4dea-bd62-9eb5ce056e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_coordinates(city, country, api_key='f6bb0876190e423ebea1880fd18f0d61'):\n",
    "    url = f'https://api.opencagedata.com/geocode/v1/json?q={city}+{country}&key={api_key}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['results']:\n",
    "        lat = data['results'][0]['geometry']['lat']\n",
    "        lng = data['results'][0]['geometry']['lng']\n",
    "        return (lat, lng)\n",
    "    return (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bc0602-02fd-4411-9f44-908c141e436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+--------+---------+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city|     lat|      lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+--------+---------+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|34.40141|-79.38644|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType, StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"lat\", FloatType(), True),\n",
    "    StructField(\"lng\", FloatType(), True)\n",
    "])\n",
    "\n",
    "fetch_coordinates_udf = udf(fetch_coordinates, schema)\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "# Select the row with missing values\n",
    "row_to_update = restaurant_df.filter(restaurant_df['lat'].isNull() & restaurant_df['lng'].isNull())\n",
    "\n",
    "# Update this row using a UDF\n",
    "updated_row = row_to_update.withColumn(\"new_coords\", fetch_coordinates_udf(row_to_update['city'], row_to_update['country']))\n",
    "updated_row = updated_row.withColumn(\"lat\", updated_row[\"new_coords\"].getItem(\"lat\")).withColumn(\"lng\", updated_row[\"new_coords\"].getItem(\"lng\"))\n",
    "updated_row = updated_row.drop(\"new_coords\")\n",
    "\n",
    "# Show the updated row\n",
    "updated_row.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66564893-d0a6-4ac4-9f75-02031fb68255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+-----------------+------------------+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city|              lat|               lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+-----------------+------------------+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|34.40140914916992|-79.38643646240234|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Collect the updated data as a list of strings\n",
    "updated_data = [row.asDict() for row in updated_row.collect()]\n",
    "\n",
    "# Update the main DataFrame\n",
    "for row_data in updated_data:\n",
    "    restaurant_df = restaurant_df.withColumn(\"lat\", when(restaurant_df[\"id\"] == row_data[\"id\"], row_data[\"lat\"]).otherwise(restaurant_df[\"lat\"]))\n",
    "    restaurant_df = restaurant_df.withColumn(\"lng\", when(restaurant_df[\"id\"] == row_data[\"id\"], row_data[\"lng\"]).otherwise(restaurant_df[\"lng\"]))\n",
    "\n",
    "# Check the updated DataFrame\n",
    "restaurant_df.filter(restaurant_df['id'] == 85899345920).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7059b0-dfa4-4113-b290-cd85068e26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+----------+----+-----+---+\n",
      "|     lng|    lat|avg_tmpr_f|avg_tmpr_c| wthr_date|year|month|day|\n",
      "+--------+-------+----------+----------+----------+----+-----+---+\n",
      "|-104.423|21.5478|      73.6|      23.1|2017-08-01|2017|    8|  1|\n",
      "|-104.374| 21.551|      72.6|      22.6|2017-08-01|2017|    8|  1|\n",
      "|-104.325|21.5541|      71.7|      22.1|2017-08-01|2017|    8|  1|\n",
      "|-104.276|21.5573|      70.9|      21.6|2017-08-01|2017|    8|  1|\n",
      "|-104.227|21.5604|      70.5|      21.4|2017-08-01|2017|    8|  1|\n",
      "|-104.178|21.5635|      70.1|      21.2|2017-08-01|2017|    8|  1|\n",
      "|-104.129|21.5665|      69.8|      21.0|2017-08-01|2017|    8|  1|\n",
      "| -104.08|21.5696|      69.5|      20.8|2017-08-01|2017|    8|  1|\n",
      "|-104.032|21.5726|      69.3|      20.7|2017-08-01|2017|    8|  1|\n",
      "|-103.983|21.5757|      71.8|      22.1|2017-08-01|2017|    8|  1|\n",
      "+--------+-------+----------+----------+----------+----+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder month=10, which contains the folders day=01, day=02, day=03, day=04\n",
    "path = \"/home/jovyan/work/PT-Spark/weather/weather\"\n",
    "\n",
    "# Reading all Parquet files from all folders day=xx in DataFrame\n",
    "weather_df = spark.read.parquet(path)\n",
    "\n",
    "# Show data\n",
    "weather_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb22352-a03b-4669-ae08-7a73907da67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as pgh\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to generate a 4-character geohash\n",
    "def generate_geohash(lat, lng):\n",
    "    return pgh.encode(lat, lng, precision=4)\n",
    "\n",
    "generate_geohash_udf = udf(generate_geohash, StringType())\n",
    "\n",
    "# Add geohash column to weather data\n",
    "weather_df = weather_df.withColumn(\"geohash\", generate_geohash_udf(\"lat\", \"lng\"))\n",
    "\n",
    "# Add geohash column to restaurant data\n",
    "restaurant_df = restaurant_df.withColumn(\"geohash\", generate_geohash_udf(\"lat\", \"lng\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2b226f-418d-4924-a6a8-dd6f38c58b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month\n",
    "# Extract month and year from the 'wthr_date' column\n",
    "weather_df = weather_df.withColumn(\"month\", month(col(\"wthr_date\")))\n",
    "weather_df = weather_df.withColumn(\"year\", year(col(\"wthr_date\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b5049-9e66-4108-a516-a75ce602a40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0164605-3a7d-4c21-8dc1-f8fffa5be65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426613\n",
      "4850979\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into three parts by month values\n",
    "months = weather_df.select('month').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create an empty DataFrame to hold the results\n",
    "final_df = spark.createDataFrame([], weather_df.schema)\n",
    "\n",
    "# Iterate through each unique month, process the data and collect the results\n",
    "for month in months:\n",
    "    # Filter the dataset by the current monthцем\n",
    "    subset_df = weather_df.filter(weather_df.month == month)\n",
    "    \n",
    "    # Remove duplicates in this subset\n",
    "    unique_subset_df = subset_df.dropDuplicates(['geohash', 'day', 'month', 'year'])\n",
    "    \n",
    "    # Add unique strings to the final result\n",
    "    final_df = final_df.union(unique_subset_df)\n",
    "    print(final_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17342c66-15e2-4e6d-9a56-114db7c141ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850979"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27f257c4-16a4-4708-8ba7-e61e337ade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'lng' and 'lat' columns from merged DataFrame\n",
    "final_cleaned_df = final_df.drop('lng', 'lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e581272c-19c6-4067-8ac1-b228441b3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join using the geohash column\n",
    "joined_df = restaurant_df.join(final_cleaned_df, on=\"geohash\", how=\"left\")\n",
    "\n",
    "# Drop duplicates if necessary to prevent data multiplication\n",
    "# You may specify subset of columns in dropDuplicates() to consider for identifying duplicates\n",
    "joined_df = joined_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc52df9-5f96-41ed-a099-f67ccfa187d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+-----------------+-----------------------+-------+-------------+------+--------+----------+----------+----------+----+-----+---+\n",
      "|geohash|          id|franchise_id|   franchise_name|restaurant_franchise_id|country|         city|   lat|     lng|avg_tmpr_f|avg_tmpr_c| wthr_date|year|month|day|\n",
      "+-------+------------+------------+-----------------+-----------------------+-------+-------------+------+--------+----------+----------+----------+----+-----+---+\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      54.7|      12.6|2017-08-03|2017|    8|  3|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      56.9|      13.8|2017-08-05|2017|    8|  5|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      57.8|      14.3|2017-08-06|2017|    8|  6|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      55.5|      13.1|2017-08-04|2017|    8|  4|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      56.0|      13.3|2017-08-02|2017|    8|  2|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      52.5|      11.4|2017-08-01|2017|    8|  1|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      56.7|      13.7|2017-08-07|2017|    8|  7|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      53.6|      12.0|2016-10-02|2016|   10|  2|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      54.5|      12.5|2016-10-03|2016|   10|  3|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      55.7|      13.2|2016-10-01|2016|   10|  1|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      54.8|      12.7|2016-10-06|2016|   10|  6|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      58.3|      14.6|2016-10-07|2016|   10|  7|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      55.8|      13.2|2016-10-04|2016|   10|  4|\n",
      "|   9prt|          25|          26| The Silver Spoon|                  47732|     US|Crescent City|41.748|-124.203|      55.1|      12.8|2016-10-05|2016|   10|  5|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      68.2|      20.1|2017-08-04|2017|    8|  4|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      63.1|      17.3|2017-08-06|2017|    8|  6|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      69.6|      20.9|2017-08-03|2017|    8|  3|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      61.1|      16.2|2017-08-01|2017|    8|  1|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      63.5|      17.5|2017-08-07|2017|    8|  7|\n",
      "|   9q96|111669149710|          15|The Pearl Kitchen|                  84488|     US|  Watsonville|36.917|-121.761|      68.0|      20.0|2017-08-02|2017|    8|  2|\n",
      "+-------+------------+------------+-----------------+-----------------------+-------+-------------+------+--------+----------+----------+----------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85eb132e-0ad2-471e-b83b-25ec1d763044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame in Parquet format with partitioning by 'year' and 'month'\n",
    "joined_df.write.partitionBy('year', 'month','day').parquet('/home/jovyan/work/PT-Spark/result/data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf82db-8808-4e84-a877-ff13cab6314e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
